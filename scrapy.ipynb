{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy란\n",
    "웹사이트에서 필요한 데이터를 추출하는 오픈소스 프레임워크.\n",
    "- 가볍고, 빠르고, 확장성이 좋다.\n",
    "    - python 기반으로 spider 코드 작성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy의 주요 특징\n",
    "- 비동기 네트워킹 라이브러리(asynchronous networking library)인 Twisted를 기반\n",
    "    - 매우 우수한 성능 발휘.\n",
    "- XPath, CSS 표현식으로 HTML 소스에서 데이터 추출이 가능.\n",
    "- webdriver를 사용하지 않음.\n",
    "- 지정된 url만 조회하므로 셀레니움보다 가볍고 빠른 퍼포먼스를 냄.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf. 셀레니움의 특징\n",
    "- Xpath, CSS 표현식으로 HTML 소스에서 데이터 추출이 가능.\n",
    "- webdriver를 사용.\n",
    "- 페이지를 렌더링 하기 위해 필요한 js, css, image 파일까지 불러옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy 예제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# quotes_spider.py\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f'quotes-{page}.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log(f'Saved file {filename}')\n",
    "\n",
    "# 크롤러를 초기화하는 과정\n",
    "if __name__ == '__main__':\n",
    "\tprocess = CrawlerProcess(setting)\n",
    "    crawler = process.create_crawler(QuotesSpider)\n",
    "    process.crawl(crawler)\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy 실행 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scrapy crawl 'spider name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### errback 코드 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "from scrapy.spidermiddlewares.httperror import HttpError\n",
    "from twisted.internet.error import DNSLookupError\n",
    "from twisted.internet.error import TimeoutError, TCPTimedOutError\n",
    "\n",
    "class ErrbackSpider(scrapy.Spider):\n",
    "    name = \"errback_example\"\n",
    "    start_urls = [\n",
    "        \"http://www.httpbin.org/\",              # HTTP 200 expected\n",
    "        \"http://www.httpbin.org/status/404\",    # Not found error\n",
    "        \"http://www.httpbin.org/status/500\",    # server issue\n",
    "        \"http://www.httpbin.org:12345/\",        # non-responding host, timeout expected\n",
    "        \"http://www.httphttpbinbin.org/\",       # DNS error expected\n",
    "    ]\n",
    "\n",
    "    # 호출이 성공하면 callback으로 선언된 parse_httpbin이 실행\n",
    "\n",
    "    def start_requests(self):\n",
    "        for u in self.start_urls:\n",
    "            yield scrapy.Request(u, callback=self.parse_httpbin,\n",
    "                                    errback=self.errback_httpbin,\n",
    "                                    dont_filter=True)\n",
    "\n",
    "    def parse_httpbin(self, response):\n",
    "        self.logger.info('Got successful response from {}'.format(response.url))\n",
    "        # do something useful here...\n",
    "\n",
    "    def errback_httpbin(self, failure):\n",
    "        # log all failures\n",
    "        self.logger.error(repr(failure))\n",
    "\n",
    "        # 호출이 실패하면 errback으로 선언된 errback_httpbin이 호출\n",
    "\n",
    "        if failure.check(HttpError):\n",
    "            # these exceptions come from HttpError spider middleware\n",
    "            # you can get the non-200 response\n",
    "            response = failure.value.response\n",
    "            self.logger.error('HttpError on %s', response.url)\n",
    "\n",
    "        elif failure.check(DNSLookupError):\n",
    "            # this is the original request\n",
    "            request = failure.request\n",
    "            self.logger.error('DNSLookupError on %s', request.url)\n",
    "\n",
    "        elif failure.check(TimeoutError, TCPTimedOutError):\n",
    "            request = failure.request\n",
    "            self.logger.error('TimeoutError on %s', request.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy 특징\n",
    "다양한 configuration 존재\n",
    "- 데이터 다운로드 타임아웃 설정.\n",
    "- 각 request간에 random한 텀(사람의 실제 액션처럼 보이기 위한)을 설정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy 장·단점\n",
    "장점\n",
    " - 확장성이 좋음.\n",
    "    - 미들웨어를 새로 개발하거나 파이프라인을 연결하는 게 쉬움.\n",
    "\n",
    "단점\n",
    "- javascript 지원이 어렵다.\n",
    "    - ajax/pjax로 데이터가 갱신되는 웹페이지라면 원하는 데이터를 추출하는 게 쉽지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf. 셀레니움 사용을 추천하는 경우\n",
    "- 한 개 사이트 안에서 여러 페이지를 돌아다니며 핸들링해야 하는 데이터가 많을 때.\n",
    "- 그 페이지가 javascript의 영향이 적을 때."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
