{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy란\n",
    "웹사이트에서 필요한 데이터를 추출하는 오픈소스 프레임워크.\n",
    "- 가볍고, 빠르고, 확장성이 좋다.\n",
    "    - python 기반으로 spider 코드 작성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy의 주요 특징\n",
    "- 비동기 네트워킹 라이브러리(asynchronous networking library)인 Twisted를 기반\n",
    "    - 매우 우수한 성능 발휘.\n",
    "- XPath, CSS 표현식으로 HTML 소스에서 데이터 추출이 가능.\n",
    "- webdriver를 사용하지 않음.\n",
    "- 지정된 url만 조회하므로 셀레니움보다 가볍고 빠른 퍼포먼스를 냄.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf. 셀레니움의 특징\n",
    "- Xpath, CSS 표현식으로 HTML 소스에서 데이터 추출이 가능.\n",
    "- webdriver를 사용.\n",
    "- 페이지를 렌더링 하기 위해 필요한 js, css, image 파일까지 불러옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy 예제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# quotes_spider.py\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f'quotes-{page}.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log(f'Saved file {filename}')\n",
    "\n",
    "# 크롤러를 초기화하는 과정\n",
    "if __name__ == '__main__':\n",
    "\tprocess = CrawlerProcess(setting)\n",
    "    crawler = process.create_crawler(QuotesSpider)\n",
    "    process.crawl(crawler)\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy 실행 코드\n",
    "```scrapy crawl 'spider name'```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### errback 코드 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "from scrapy.spidermiddlewares.httperror import HttpError\n",
    "from twisted.internet.error import DNSLookupError\n",
    "from twisted.internet.error import TimeoutError, TCPTimedOutError\n",
    "\n",
    "class ErrbackSpider(scrapy.Spider):\n",
    "    name = \"errback_example\"\n",
    "    start_urls = [\n",
    "        \"http://www.httpbin.org/\",              # HTTP 200 expected\n",
    "        \"http://www.httpbin.org/status/404\",    # Not found error\n",
    "        \"http://www.httpbin.org/status/500\",    # server issue\n",
    "        \"http://www.httpbin.org:12345/\",        # non-responding host, timeout expected\n",
    "        \"http://www.httphttpbinbin.org/\",       # DNS error expected\n",
    "    ]\n",
    "\n",
    "    # 호출이 성공하면 callback으로 선언된 parse_httpbin이 실행\n",
    "\n",
    "    def start_requests(self):\n",
    "        for u in self.start_urls:\n",
    "            yield scrapy.Request(u, callback=self.parse_httpbin,\n",
    "                                    errback=self.errback_httpbin,\n",
    "                                    dont_filter=True)\n",
    "\n",
    "    def parse_httpbin(self, response):\n",
    "        self.logger.info('Got successful response from {}'.format(response.url))\n",
    "        # do something useful here...\n",
    "\n",
    "    def errback_httpbin(self, failure):\n",
    "        # log all failures\n",
    "        self.logger.error(repr(failure))\n",
    "\n",
    "        # 호출이 실패하면 errback으로 선언된 errback_httpbin이 호출\n",
    "\n",
    "        if failure.check(HttpError):\n",
    "            # these exceptions come from HttpError spider middleware\n",
    "            # you can get the non-200 response\n",
    "            response = failure.value.response\n",
    "            self.logger.error('HttpError on %s', response.url)\n",
    "\n",
    "        elif failure.check(DNSLookupError):\n",
    "            # this is the original request\n",
    "            request = failure.request\n",
    "            self.logger.error('DNSLookupError on %s', request.url)\n",
    "\n",
    "        elif failure.check(TimeoutError, TCPTimedOutError):\n",
    "            request = failure.request\n",
    "            self.logger.error('TimeoutError on %s', request.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapy 특징\n",
    "다양한 configuration 존재\n",
    "- 데이터 다운로드 타임아웃 설정.\n",
    "- 각 request간에 random한 텀(사람의 실제 액션처럼 보이기 위한)을 설정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Scrapy 장·단점\n",
    "장점\n",
    " - 확장성이 좋음.\n",
    "    - 미들웨어를 새로 개발하거나 파이프라인을 연결하는 게 쉬움.\n",
    "\n",
    "단점\n",
    "- javascript 지원이 어렵다.\n",
    "    - ajax/pjax로 데이터가 갱신되는 웹페이지라면 원하는 데이터를 추출하는 게 쉽지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cf. 셀레니움 사용을 추천하는 경우\n",
    "- 한 개 사이트 안에서 여러 페이지를 돌아다니며 핸들링해야 하는 데이터가 많을 때.\n",
    "- 그 페이지가 javascript의 영향이 적을 때."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy 시작하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### scrapy\n",
    "- python 기반의 web crawling 라이브러리.\n",
    "\n",
    "scrapy shell 사용\n",
    "    ```scrapy shell \"<url주소>\"```\n",
    "\n",
    "url을 메모리에 저장\n",
    "    ```fetch(\"<url주소>\")```\n",
    "\n",
    "url 창 보이기\n",
    "    ```view(response)```\n",
    "\n",
    "html 코드 보이기(페이지 소스)\n",
    "    ```print(response.text)```\n",
    "\n",
    "xpath를 통해 경로 데이터 가져오기\n",
    "    ```response.xpath(\"<xpath>/text()\").extract()```\n",
    "- li[1] ... li[10] 반복이면 li로 입력.\n",
    "\n",
    "css(스타일시트)를 통해 데이터 추출\n",
    "- span class=\"writing\"의 text를 추출\n",
    "    ```response.css(\".writing::text\").extract()```\n",
    "- span class=\"lede\"의 text를 추출\n",
    "    ```response.css(\".lede::text\").extract()```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### scrapy project\n",
    "scrapy를 이용한 네이버 뉴스 데이터 크롤링\n",
    "\n",
    "scrapy 프로젝트 생성\n",
    "    ```scrapy startproject <프로젝트명>```\n",
    "\n",
    "프로젝트 디렉토리로 이동\n",
    "    ```cd <프로젝트명>```\n",
    "\n",
    "visual studio code 시작\n",
    "    ```code .```\n",
    "\n",
    "spider 생성\n",
    "    ```scrapy genspider <spider name> <bot을 만들 domain 주소>```\n",
    "    - https:// 도메인 빼고 작성\n",
    "\n",
    "/mybot.py\n",
    "```\n",
    "# domain 정의 (네이버 뉴스)\n",
    "  allowed_domains = ['naver.com']\n",
    "  start_urls = ['http://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid1=105&sid2=226/']\n",
    "  ```\n",
    "\n",
    "/items.py\n",
    "```\n",
    "# news title, writer, preview 추출\n",
    "  class MyscraperItem(scrapy.Item):\n",
    "      title = scrapy.Field()\n",
    "      writer = scrapy.Field()\n",
    "      preview = scrapy.Field() # 필드 정의\n",
    " ```\n",
    "\n",
    "/mybot.py\n",
    "```\n",
    " from myscraper.items import MyscraperItem\n",
    "  \n",
    "  def parse():        \n",
    "      titles = response.xpath('//*[@id=\"main_content\"]/div[2]/ul[1]/li/dl/dt[2]/a/text()').extract()\n",
    "      writers = response.css('.writing::text').extract()\n",
    "      previews = response.css('.lede::text').extract()\n",
    "      \n",
    "      items = []\n",
    "      # items에 XPATH, CSS를 통해 추출한 데이터를 저장\n",
    "      for idx in range(len(titles)):\n",
    "          item = MyscraperItem()\n",
    "          item['title'] = titles[idx]\n",
    "          item['writer'] = writers[idx]\n",
    "          item['preview'] = previews[idx]\n",
    "          \n",
    "          items.append(item)\n",
    "  \n",
    "      return items\n",
    "```\n",
    "\n",
    "/settings.py\n",
    "```\n",
    "  # 웹스크래핑 시 로봇의 수정 및 동작을 막음\n",
    "  ROBOTSTXT_OBEY = False\n",
    "  \n",
    "  # csv 파일 생성\n",
    "  # FEED_FORMAT = \"csv\"\n",
    "  # FEED_URI = \"my_news.csv\"\n",
    "  FEED_FORMAT = \"json\"\n",
    "  FEED_URI = \"my_news.json\"\n",
    "  FEED_EXPORT_ENCODING = \"utf-8 sig\"\n",
    "```\n",
    "\n",
    "paging\n",
    "- 여러 페이지 목록을 한번에 스크래핑\n",
    "```\n",
    "from scrapy.http import Request\n",
    "\n",
    "URL = 'http://movie.naver.com/movie/point/af/list.nhn&page=%s'\n",
    "start_page = 1\n",
    "\n",
    "class MybotSpider(scrapy.Spider):\n",
    "  name = 'mybot'\n",
    "  allowed_domains = ['naver.com']\n",
    "  start_urls = [URL % start_page]\n",
    "\n",
    "  def start_requests(self):\n",
    "      for i in range(6):  # 0 ~ 5\n",
    "          yield Request(url=URL & (i + start_page), callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### scrapy 실행\n",
    "스크래핑 실행\n",
    "    ```scrapy crawl <bot 이름>```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
